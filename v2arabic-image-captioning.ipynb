{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8251710,"sourceType":"datasetVersion","datasetId":4895142},{"sourceId":14861039,"sourceType":"datasetVersion","datasetId":9506472}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T18:40:10.566282Z","iopub.execute_input":"2026-02-16T18:40:10.566811Z","iopub.status.idle":"2026-02-16T18:40:15.165360Z","shell.execute_reply.started":"2026-02-16T18:40:10.566765Z","shell.execute_reply":"2026-02-16T18:40:15.164598Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (26.0rc2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"! pip install arabic_reshaper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T18:40:15.166935Z","iopub.execute_input":"2026-02-16T18:40:15.167161Z","iopub.status.idle":"2026-02-16T18:40:18.461972Z","shell.execute_reply.started":"2026-02-16T18:40:15.167135Z","shell.execute_reply":"2026-02-16T18:40:18.461258Z"}},"outputs":[{"name":"stdout","text":"Collecting arabic_reshaper\n  Downloading arabic_reshaper-3.0.0-py3-none-any.whl.metadata (12 kB)\nDownloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\nInstalling collected packages: arabic_reshaper\nSuccessfully installed arabic_reshaper-3.0.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ==========================================\n# üöÄ FINAL HYBRID MASTER CELL: ViT + AraGPT2\n# Tweak: Added Dropout & Regularization to match Old Code\n# ==========================================\nimport os\nimport re\nimport json\nimport time\nimport torch\nimport pandas as pd\nimport numpy as np\nimport random\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport evaluate\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n\n# üõ†Ô∏è IMPORTS (Old & New Mixed)\nfrom torchvision import transforms\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nfrom torch.optim import AdamW\nimport arabic_reshaper\nfrom bidi.algorithm import get_display\n\n# 0. CONFIGURATION\n# ---------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üöÄ Using Device: {device}\")\n\n# üö® PATHS\nTXT_FILE_PATH = '/kaggle/input/datasets/samahfetouh/arabic-flickr8k-dataset/captions.txt'\nIMG_ROOT_DIR = '/kaggle/input/datasets/samahfetouh/arabic-flickr8k-dataset/Images'\n\n# Config\nENCODER_CHECKPOINT = \"google/vit-base-patch16-224-in21k\"\nDECODER_CHECKPOINT = \"aubmindlab/aragpt2-base\"\nOUTPUT_DIR = \"./flickr8k_hybrid_model\"\nLOG_FILE = \"training_log.csv\"\n\n# üß† HYPERPARAMETERS (Matched to Old Code)\nMAX_LENGTH = 32         \nBATCH_SIZE = 32         \nFINE_TUNE_AT_EPOCH = 8  # üîí The \"Old Code\" Magic Number\nTOTAL_EPOCHS = 20       \nDROPOUT_RATE = 0.3      # ‚¨ÜÔ∏è Increased from 0.1 to match Old Code's robustness\n\n# üõ†Ô∏è HELPER: ARABIC NORMALIZATION\ndef normalize_arabic(text):\n    text = re.sub(r\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n    text = re.sub(r\"ÿ©\", \"Ÿá\", text)\n    text = re.sub(r\"Ÿâ\", \"Ÿä\", text)\n    text = re.sub(r\"_\", \" \", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text.strip()\n\n# 1. ROBUST DATA LOADING\n# ---------------------------------------------------------\ndef load_data_aggressive(txt_path, img_root):\n    image_map = {}\n    for root, dirs, files in os.walk(img_root):\n        for f in files:\n            if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n                key = os.path.splitext(f)[0].strip()\n                image_map[key] = os.path.join(root, f)\n    \n    if not image_map: raise ValueError(\"‚ùå No images found!\")\n    \n    data = []\n    with open(txt_path, 'r', encoding='utf-8') as f: lines = f.readlines()\n    \n    for line in lines:\n        line = line.strip()\n        if len(line) < 5: continue\n        parts = re.split(r'[,\\t]', line, maxsplit=1)\n        if len(parts) < 2: continue\n        \n        img_key = parts[0].strip().split('#')[0]\n        img_key = os.path.splitext(img_key)[0].strip()\n        caption = parts[1].strip()\n        \n        if img_key in image_map:\n            data.append({'image_path': image_map[img_key], 'caption': caption})\n            \n    return pd.DataFrame(data)\n\nprint(\"‚è≥ Loading Data...\")\nfull_df = load_data_aggressive(TXT_FILE_PATH, IMG_ROOT_DIR)\nprint(f\"‚úÖ Loaded {len(full_df)} pairs.\")\n\ntrain_df, val_df = train_test_split(full_df, test_size=0.1, random_state=42)\n\n# 2. DATASET WITH AUGMENTATION (The \"Old Code\" Secret)\n# ---------------------------------------------------------\nfeature_extractor = ViTImageProcessor.from_pretrained(ENCODER_CHECKPOINT)\ntokenizer = AutoTokenizer.from_pretrained(DECODER_CHECKPOINT)\ntokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': '[BOS]'})\n\n# üé® AUGMENTATION: Random Flips + Noise (Crucial for Small Datasets)\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5), # 50% chance to flip\n    transforms.ColorJitter(brightness=0.1, contrast=0.1), \n    transforms.ToTensor()\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\nclass Flickr8kDataset(Dataset):\n    def __init__(self, df, tokenizer, transform):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.transform = transform \n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['image_path']).convert(\"RGB\")\n        \n        # Apply Torchvision Augmentation\n        img_tensor = self.transform(image) \n        \n        # Convert to HuggingFace Pixel Values\n        # do_rescale=False because ToTensor() already scales to [0,1]\n        pixel_values = feature_extractor(images=img_tensor, return_tensors=\"pt\", do_rescale=False).pixel_values.squeeze()\n        \n        caption = normalize_arabic(row['caption'])\n        raw_tokens = self.tokenizer(caption, add_special_tokens=False).input_ids\n        if len(raw_tokens) > MAX_LENGTH - 2: raw_tokens = raw_tokens[:MAX_LENGTH - 2]\n        final_tokens = [self.tokenizer.bos_token_id] + raw_tokens + [self.tokenizer.eos_token_id]\n        \n        padding_len = MAX_LENGTH - len(final_tokens)\n        if padding_len > 0:\n            final_tokens = final_tokens + [self.tokenizer.pad_token_id] * padding_len\n            \n        labels = torch.tensor(final_tokens)\n        labels[labels == self.tokenizer.pad_token_id] = -100 \n        \n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ntrain_ds = Flickr8kDataset(train_df, tokenizer, train_transforms)\nval_ds = Flickr8kDataset(val_df, tokenizer, val_transforms)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# 3. MODEL INIT (With High Dropout)\n# ---------------------------------------------------------\nprint(\"üèóÔ∏è Initializing Model...\")\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(ENCODER_CHECKPOINT, DECODER_CHECKPOINT)\nmodel.decoder.resize_token_embeddings(len(tokenizer))\n\n# üõ†Ô∏è INJECTING DROPOUT (The \"Old Code\" Stability Fix)\n# We force the model to use higher dropout to prevent memorization\nmodel.config.decoder.activation_dropout = DROPOUT_RATE\nmodel.config.decoder.attn_pdrop = DROPOUT_RATE\nmodel.config.decoder.embd_pdrop = DROPOUT_RATE\nprint(f\"üõ°Ô∏è Dropout set to {DROPOUT_RATE} (Matching Old Code)\")\n\n# Generation Config\nmodel.config.num_beams = 3\nmodel.config.max_length = MAX_LENGTH\nmodel.config.early_stopping = True \nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id \nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.vocab_size = len(tokenizer)\n\nmodel.to(device)\n\n# 4. TRAINING ENGINE\n# ---------------------------------------------------------\n# CSV Logger\nif not os.path.exists(LOG_FILE):\n    with open(LOG_FILE, \"w\") as f: f.write(\"epoch,train_loss,val_bleu1,val_bleu4,saved\\n\")\n\ndef save_log(epoch, train_loss, bleu1, bleu4, saved):\n    with open(LOG_FILE, \"a\") as f:\n        f.write(f\"{epoch},{train_loss:.4f},{bleu1:.2f},{bleu4:.2f},{saved}\\n\")\n\noptimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5, weight_decay=0.01) # Added Weight Decay\nbest_bleu = 0.0\n\nprint(f\"üöÄ Starting {TOTAL_EPOCHS} Epoch Training Scheme...\")\n\nfor epoch in range(1, TOTAL_EPOCHS + 1):\n    \n    # --- PHASE SWITCHER ---\n    if epoch == 1:\n        print(\"‚ùÑÔ∏è PHASE 1: Encoder Frozen (Epochs 1-8)\")\n        for param in model.encoder.parameters(): param.requires_grad = False\n        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5, weight_decay=0.01)\n        \n    elif epoch == FINE_TUNE_AT_EPOCH:\n        print(\"üîì PHASE 2: Unfreezing Encoder (Epoch 8+)\")\n        print(\"   -> Lowering LR to prevent catastrophic forgetting\")\n        for param in model.encoder.parameters(): param.requires_grad = True\n        optimizer = AdamW([\n            {'params': model.decoder.parameters(), 'lr': 5e-5, 'weight_decay': 0.01}, \n            {'params': model.encoder.parameters(), 'lr': 1e-5, 'weight_decay': 0.01}  \n        ])\n\n    # --- TRAIN ---\n    model.train()\n    total_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{TOTAL_EPOCHS}\")\n    for batch in loop:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        loss = outputs.loss\n        optimizer.zero_grad(); loss.backward(); optimizer.step()\n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    avg_train_loss = total_loss / len(train_loader)\n\n    # --- VALIDATE ---\n    model.eval()\n    image_map = {}\n    for _, row in val_df.iterrows():\n        if row['image_path'] not in image_map: image_map[row['image_path']] = []\n        image_map[row['image_path']].append(row['caption'])\n    \n    eval_imgs = list(image_map.keys())[:300]\n    references, hypotheses = [], []\n    \n    print(\"‚è≥ Validating...\")\n    with torch.no_grad():\n        for img_path in tqdm(eval_imgs, desc=\"Eval\", leave=False):\n            try:\n                image = Image.open(img_path).convert(\"RGB\")\n                img_tensor = val_transforms(image)\n                pixel_values = feature_extractor(images=img_tensor, return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n            except: continue\n            \n            gen_ids = model.generate(pixel_values, max_new_tokens=30)\n            pred = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n            hypotheses.append(normalize_arabic(pred).split())\n            references.append([normalize_arabic(c).split() for c in image_map[img_path]])\n\n    chencherry = SmoothingFunction()\n    bleu4 = corpus_bleu(references, hypotheses, smoothing_function=chencherry.method4) * 100\n    bleu1 = corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0), smoothing_function=chencherry.method4) * 100\n    \n    print(f\"üìâ Epoch {epoch} Results:\")\n    print(f\"   ‚Ä¢ Train Loss: {avg_train_loss:.4f}\")\n    print(f\"   ‚Ä¢ BLEU-4:     {bleu4:.2f} (Target: >14.0)\")\n    \n    # --- SAVE LOGIC ---\n    saved_status = False\n    if bleu4 > best_bleu:\n        best_bleu = bleu4\n        print(f\"‚≠ê New Best Model! Saving to {OUTPUT_DIR}...\")\n        model.save_pretrained(OUTPUT_DIR)\n        tokenizer.save_pretrained(OUTPUT_DIR)\n        feature_extractor.save_pretrained(OUTPUT_DIR)\n        saved_status = True\n        \n    save_log(epoch, avg_train_loss, bleu1, bleu4, saved_status)\n\nprint(f\"‚úÖ Training Complete. Best BLEU-4: {best_bleu:.2f}\")\nprint(f\"üìÑ Log saved to: {LOG_FILE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T08:41:22.089249Z","iopub.execute_input":"2026-02-16T08:41:22.090031Z","iopub.status.idle":"2026-02-16T08:41:57.608555Z","shell.execute_reply.started":"2026-02-16T08:41:22.089992Z","shell.execute_reply":"2026-02-16T08:41:57.607458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# üöÄ FLICKR8K FINAL MASTER CELL\n# Strategy: Aggressive Regularization to stop Overfitting\n# ==========================================\nimport os\nimport re\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nimport random\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nimport evaluate\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n\nfrom torchvision import transforms\nfrom transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\nfrom torch.optim import AdamW\nimport arabic_reshaper\nfrom bidi.algorithm import get_display\n\n# 0. CONFIGURATION\n# ---------------------------------------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üöÄ Using Device: {device}\")\n\n# üö® PATHS\nTXT_FILE_PATH = '/kaggle/input/datasets/samahfetouh/arabic-flickr8k-dataset/captions.txt'\nIMG_ROOT_DIR = '/kaggle/input/datasets/samahfetouh/arabic-flickr8k-dataset/Images'\n\nENCODER_CHECKPOINT = \"google/vit-base-patch16-224-in21k\"\nDECODER_CHECKPOINT = \"aubmindlab/aragpt2-base\"\nOUTPUT_DIR = \"./flickr8k_regularized\"\n\n# üõ†Ô∏è TUNED HYPERPARAMETERS\nMAX_LENGTH = 32         \nBATCH_SIZE = 32         \nEPOCHS = 15             \nLEARNING_RATE = 2e-5    # Lower LR to prevent memorization\nWEIGHT_DECAY = 0.05     # High decay to punish complexity\nLABEL_SMOOTHING = 0.1   # Prevent model from being \"too sure\"\n\n# 1. ROBUST DATA LOADER\n# ---------------------------------------------------------\ndef normalize_arabic(text):\n    text = re.sub(r\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n    text = re.sub(r\"ÿ©\", \"Ÿá\", text)\n    text = re.sub(r\"Ÿâ\", \"Ÿä\", text)\n    text = re.sub(r\"_\", \" \", text)\n    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n    return text.strip()\n\ndef load_data_aggressive(txt_path, img_root):\n    image_map = {}\n    for root, dirs, files in os.walk(img_root):\n        for f in files:\n            if f.lower().endswith(('.jpg', '.jpeg', '.png')):\n                key = os.path.splitext(f)[0].strip()\n                image_map[key] = os.path.join(root, f)\n    \n    data = []\n    with open(txt_path, 'r', encoding='utf-8') as f: lines = f.readlines()\n    for line in lines:\n        line = line.strip()\n        if len(line) < 5: continue\n        parts = re.split(r'[,\\t]', line, maxsplit=1)\n        if len(parts) < 2: continue\n        img_key = parts[0].strip().split('#')[0]\n        img_key = os.path.splitext(img_key)[0].strip()\n        if img_key in image_map:\n            data.append({'image_path': image_map[img_key], 'caption': parts[1].strip()})\n    return pd.DataFrame(data)\n\nprint(\"‚è≥ Loading Data...\")\nfull_df = load_data_aggressive(TXT_FILE_PATH, IMG_ROOT_DIR)\ntrain_df, val_df = train_test_split(full_df, test_size=0.1, random_state=42)\nprint(f\"‚úÖ Train: {len(train_df)} | Val: {len(val_df)}\")\n\n# 2. DATASET (WITH STRONG AUGMENTATION)\n# ---------------------------------------------------------\nfeature_extractor = ViTImageProcessor.from_pretrained(ENCODER_CHECKPOINT)\ntokenizer = AutoTokenizer.from_pretrained(DECODER_CHECKPOINT)\ntokenizer.add_special_tokens({'pad_token': '[PAD]', 'bos_token': '[BOS]'})\n\n# üé® AGGRESSIVE AUGMENTATION\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)), # Zoom in randomly\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10), # Slight rotation\n    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n    transforms.ToTensor()\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\nclass Flickr8kDataset(Dataset):\n    def __init__(self, df, tokenizer, transform):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.transform = transform \n\n    def __len__(self): return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = Image.open(row['image_path']).convert(\"RGB\")\n        img_tensor = self.transform(image)\n        # do_rescale=False because ToTensor() already scales to [0,1]\n        pixel_values = feature_extractor(images=img_tensor, return_tensors=\"pt\", do_rescale=False).pixel_values.squeeze()\n        \n        caption = normalize_arabic(row['caption'])\n        raw_tokens = self.tokenizer(caption, add_special_tokens=False).input_ids\n        if len(raw_tokens) > MAX_LENGTH - 2: raw_tokens = raw_tokens[:MAX_LENGTH - 2]\n        final_tokens = [self.tokenizer.bos_token_id] + raw_tokens + [self.tokenizer.eos_token_id]\n        \n        padding_len = MAX_LENGTH - len(final_tokens)\n        if padding_len > 0:\n            final_tokens = final_tokens + [self.tokenizer.pad_token_id] * padding_len\n            \n        labels = torch.tensor(final_tokens)\n        labels[labels == self.tokenizer.pad_token_id] = -100 \n        \n        return {\"pixel_values\": pixel_values, \"labels\": labels}\n\ntrain_ds = Flickr8kDataset(train_df, tokenizer, train_transforms)\nval_ds = Flickr8kDataset(val_df, tokenizer, val_transforms)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\n# 3. MODEL INIT (PARTIAL FREEZE)\n# ---------------------------------------------------------\nprint(\"üèóÔ∏è Initializing Model...\")\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(ENCODER_CHECKPOINT, DECODER_CHECKPOINT)\nmodel.decoder.resize_token_embeddings(len(tokenizer))\n\nmodel.config.num_beams = 4\nmodel.config.max_length = MAX_LENGTH\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id \nmodel.config.eos_token_id = tokenizer.eos_token_id\nmodel.config.vocab_size = len(tokenizer)\n\n# üîí FREEZE STRATEGY\n# 1. Freeze ViT (Encoder) entirely first\nfor param in model.encoder.parameters(): param.requires_grad = False\n# 2. Freeze Bottom 6 Layers of AraGPT2 (Decoder) - Keep basic language skills\nfor i, block in enumerate(model.decoder.transformer.h):\n    if i < 6: # AraGPT2-base has 12 layers, freeze first half\n        for param in block.parameters(): param.requires_grad = False\n\nprint(\"‚ùÑÔ∏è Frozen: ViT Encoder + Bottom 6 Decoder Layers\")\n\nmodel.to(device)\n\n# 4. TRAINING WITH LABEL SMOOTHING\n# ---------------------------------------------------------\noptimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n# Custom Loss with Label Smoothing\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=LABEL_SMOOTHING)\n\nbest_bleu = 0.0\nhistory = []\n\nprint(f\"üöÄ Starting Training ({EPOCHS} Epochs)...\")\n\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n    \n    for batch in loop:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        \n        # Forward pass (get logits, not loss directly, so we can smooth)\n        outputs = model(pixel_values=pixel_values, labels=labels)\n        logits = outputs.logits\n        \n        # Reshape for Loss: (Batch * Seq, Vocab)\n        loss = criterion(logits.view(-1, model.config.vocab_size), labels.view(-1))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    avg_loss = total_loss / len(train_loader)\n    \n    # VALIDATE (Every 2 epochs to save time, or every epoch)\n    model.eval()\n    image_map = {}\n    for _, row in val_df.iterrows():\n        if row['image_path'] not in image_map: image_map[row['image_path']] = []\n        image_map[row['image_path']].append(row['caption'])\n    \n    eval_imgs = list(image_map.keys())[:300]\n    references, hypotheses = [], []\n    \n    print(\"‚è≥ Validating...\")\n    with torch.no_grad():\n        for img_path in tqdm(eval_imgs, desc=\"Eval\", leave=False):\n            try:\n                image = Image.open(img_path).convert(\"RGB\")\n                img_tensor = val_transforms(image)\n                pixel_values = feature_extractor(images=img_tensor, return_tensors=\"pt\", do_rescale=False).pixel_values.to(device)\n            except: continue\n            \n            gen_ids = model.generate(pixel_values, max_new_tokens=30)\n            pred = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n            hypotheses.append(normalize_arabic(pred).split())\n            references.append([normalize_arabic(c).split() for c in image_map[img_path]])\n\n    chencherry = SmoothingFunction()\n    bleu1 = corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0), smoothing_function=chencherry.method4) * 100\n    bleu4 = corpus_bleu(references, hypotheses, smoothing_function=chencherry.method4) * 100\n    \n    print(f\"üìâ Epoch {epoch}: Loss={avg_loss:.4f} | BLEU-1={bleu1:.2f} | BLEU-4={bleu4:.2f}\")\n    \n    if bleu1 > best_bleu:\n        best_bleu = bleu1\n        print(f\"‚≠ê Saving Best Model (BLEU-1: {best_bleu:.2f})\")\n        model.save_pretrained(OUTPUT_DIR)\n        tokenizer.save_pretrained(OUTPUT_DIR)\n        feature_extractor.save_pretrained(OUTPUT_DIR)\n\nprint(\"‚úÖ DONE!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-16T18:46:26.058784Z","iopub.execute_input":"2026-02-16T18:46:26.059503Z","iopub.status.idle":"2026-02-16T21:30:54.172908Z","shell.execute_reply.started":"2026-02-16T18:46:26.059466Z","shell.execute_reply":"2026-02-16T21:30:54.172039Z"}},"outputs":[{"name":"stdout","text":"üöÄ Using Device: cuda\n‚è≥ Loading Data...\n‚úÖ Train: 21845 | Val: 2428\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d48018ae2f45879eb6e34afb2fffaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73312325a3e24869a38662f4141642d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3708b63a0c1f470a82e76656f208a830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"098967c4c3c94a3da33e66a41616b872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf473facee4b4b9f94596ae131d3f81b"}},"metadata":{}},{"name":"stdout","text":"üèóÔ∏è Initializing Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceba285638cc48939a2bd4a9548f8d28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"432edb89bc1344f3826b0fd4f7444043"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6e36e3f84a54090b2e095e525c1145c"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at aubmindlab/aragpt2-base and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"‚ùÑÔ∏è Frozen: ViT Encoder + Bottom 6 Decoder Layers\nüöÄ Starting Training (15 Epochs)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1468084b90034f2d999ec76b8790de23"}},"metadata":{}},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"üìâ Epoch 1: Loss=6.9931 | BLEU-1=7.36 | BLEU-4=0.71\n‚≠ê Saving Best Model (BLEU-1: 7.36)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 32, 'num_beams': 4}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50beafd6bd14e56bf81045fd4c96512"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 2: Loss=5.1785 | BLEU-1=14.27 | BLEU-4=1.24\n‚≠ê Saving Best Model (BLEU-1: 14.27)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"600c47f5506e46e99b1668bcf27cffa8"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 3: Loss=4.6241 | BLEU-1=16.00 | BLEU-4=1.36\n‚≠ê Saving Best Model (BLEU-1: 16.00)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce323accd2384696a58794e652d41cea"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 4: Loss=4.4148 | BLEU-1=17.93 | BLEU-4=2.62\n‚≠ê Saving Best Model (BLEU-1: 17.93)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9a2e0d5c424edf890678dec6443aac"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 5: Loss=4.2629 | BLEU-1=18.71 | BLEU-4=3.46\n‚≠ê Saving Best Model (BLEU-1: 18.71)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e46f2532ab8475fa3f278f0ffdb0750"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 6: Loss=4.1382 | BLEU-1=20.86 | BLEU-4=3.82\n‚≠ê Saving Best Model (BLEU-1: 20.86)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"923694ee3dc84e46b250ff3d9b990a84"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 7: Loss=4.0395 | BLEU-1=22.66 | BLEU-4=4.55\n‚≠ê Saving Best Model (BLEU-1: 22.66)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cde707ec513c4f3f85728fd96ec8d34a"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 8: Loss=3.9588 | BLEU-1=22.53 | BLEU-4=4.90\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0176b8e14794934a563a3317ac5dab0"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 9: Loss=3.8900 | BLEU-1=22.66 | BLEU-4=4.60\n‚≠ê Saving Best Model (BLEU-1: 22.66)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cfc1582cf4e417eb2d3492560aa974b"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98de582e89234194bf93bf9803ded655"}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 10: Loss=3.8237 | BLEU-1=22.68 | BLEU-4=4.99\n‚≠ê Saving Best Model (BLEU-1: 22.68)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe00ad54d58a41db82f7e7cd21f77df2"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 11: Loss=3.7701 | BLEU-1=23.25 | BLEU-4=5.36\n‚≠ê Saving Best Model (BLEU-1: 23.25)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a2f6d49e77416bbacb830b4cd58a60"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 12: Loss=3.7189 | BLEU-1=24.64 | BLEU-4=5.90\n‚≠ê Saving Best Model (BLEU-1: 24.64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d013ee8dcfac43fe9462199447bf0b37"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 13: Loss=3.6703 | BLEU-1=24.92 | BLEU-4=5.94\n‚≠ê Saving Best Model (BLEU-1: 24.92)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c40e99044424ac3bac9fc4f1cd502cb"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 14: Loss=3.6280 | BLEU-1=24.77 | BLEU-4=6.61\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/15:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e5f8c9a53648ed8858b03ab394061e"}},"metadata":{}},{"name":"stdout","text":"‚è≥ Validating...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"üìâ Epoch 15: Loss=3.5865 | BLEU-1=24.70 | BLEU-4=7.03\n‚úÖ DONE!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}